# zhongruanbei
中软杯的最终作品

第七届中软杯赛题十一：企业信息族谱分析
本系统实现在linux集群中对爬虫数据进行离线的分析，通过sqoop实现数据的定期把hbase中的数据同步到mysql，实现服务器数据的实时性


1.技术背景
企业信用数据来源主要是国家企业信用公示系统，该网站反爬虫机制极为强大。Selenium自动化测试框架目前被广泛应用于反反爬虫，爬虫直接调用浏览器模拟人的操作进行数据获取，俗称浏览器爬虫。而Yolo（You only look once）是目前应用最广的目标检测神经网络之一，模型准确率高且速度极快，在本系统中用于破解国家企业信用公示系统的验证码从而获取数据。
服务器框架主要由数据交互以及离线处理构成，对于离线处理我们部署了spark,hbase和hadoop集群。用于对大量数据的快速分析处理，数据交互服务器的编写采用ssh框架的规范，实现快速开发，也便于后期的维护。
2.任务概述
  2.1技术目标
  C-query系统旨在通过爬虫从国家企业信用公示系统获取尽可能详细的企业信用数据，据此向用户提供市场主体检索、市场主体基本信息、投资族谱、疑似关系等众多功能。解决企业信息不够透明、关联度低，投资者无法找到最可靠合作伙伴等问题 基本技术目标包括：
  
  （1）支持系统后台管理员通过爬虫获取到国家企业信用公示系统的企业各项基本信息。
  
  （2）支持sqoop周期性的进行hbase和mysql的离线数据传输以及spark的离线数据分析。
  
  （3）支持提供市场主体登记基本信息查询，输入关键字后支持联想功能以及模糊搜索的功能。
  
  （4）支持展现该主体各股东出资比例，并标记出最大股东。
  
  （5）支持提供市场主体对外投资、股东再投资族谱。投资族谱可逐级进行延伸。
  
  （6）支持提取市场主体股东信息以及对外投资信息，挖掘企业之间的相互关联。
  
  （7）提供该主体主要人员对外投资信息及主要人员在外任职信息。

  2.2运行环境
    2.2.1用户端配置
    C-query系统用户端所需的基本配置如下：
    (1)、配置安装了各大主流浏览器之一
    (2)、网络带宽速度在1M及以上
    (3)、操作系统无限制
    2.2.2服务器端配置
    C-query系统服务器端所需配置如下：
    1、服务器：tomcat6及以上
    2、操作系统：windows（信息交互服务器）、Linux（centos7，数据获取及离线数据分析服务器）
    3、数据库：Hbase 1.2及以上、Mysql 5.6及以上
    4、基本配置：python2.7、Spark2.2.0（Built for Hadoop2.2.0）分布式环境、 Sqoop1.4及以上、JDK1.7及以上、Scala2.12.6及以上、配置深度学习环境（CUDA等）、安装火狐浏览器以及geckodriver插件。

  2.3功能需求概述
    C-query系统功能需求包括企业基本信息检索，股权结构、投资图谱、企业图谱以及疑似关系的展示，界面友好，查询时间在两秒以内。
    2.3.1 企业基本信息信息检索
    提供市场主体登记基本信息查询，需提供检索入口，输入关键字后需支持联想功能，查询展示的字段项包括：企业名称、法定代表人、成立日期、注册资本、企业类型、统一社会信用代码、经营范围、公司地址、登记机关等。
    2.3.2 股权结构展示
    展现该主体各股东出资比例，并标记出最大股东、企业股东、自然人股东。
    2.3.3 投资图谱展示
    提供市场主体对外投资、股东再投资族谱。投资族谱可逐级进行延伸。
    2.3.4 企业图谱展示
    提取市场主体股东信息以及对外投资信息，挖掘企业之间的相互关联，展现企业族谱。企业族谱可向上逐级展示一级、二级、三级……,向下可层层延伸至分公司。
    2.3.5 疑似关系展示
    提供该主体主要人员对外投资信息以及主要人员在外任职信息。

  2.4性能需求概述
    2.4.1 企业信息数据库容量
    C-query系统预期的存储的公司的存储容量应大于等于十万，以覆盖中国所有企业 。
    2.4.2 检索及图像生成速率
    普通检索、高级检索和图谱生成的速度在两秒以内。

  2.5条件与限制
  C-query系统使用的条件与限制主要有：用户端需要通过网络连接本系统的服务器端，可能需要较好的网络环境才能获得较好的用户体验。

  3.3爬虫运作流程

  我们首先利用python爬虫从互联网上爬取企业名录并存到数据库中，然后将这些企业名录应用于国家企业信用信息公示系统的爬取，我们采用了搭载了selenium测试框架的浏览器爬虫，通过darknet神经网络，训练深度学习模型破解验证码，然后爬取数据并存到数据库中。

  3.4数据分析过程
  在数据分析部分，我们用spark读取hbase中的原始爬虫数据，我们离线分析分为两个部分。首先是数据清洗对于残缺数据，残缺度较小的时候我们会自动补全空缺项，缺度过大的时，接删除记录，避免影响以后的分析，对于重复数据，我们根据注册号，也就是公司的主键来去重。数据分析提取部分，我们主要以正则表达式进行数据的提取，分析中将hbase数据读取到rdd通过rdd的一系列转换算子，再加上sparksql的使用完成我们的数据分析

4.详细设计与实现
  4.1后台企业数据获取模块（爬虫）设计
    4.1.1模块设计原则
       企业数据获取模块设计遵循以下原则：
    （1）确保数据的完整性，正确性
    （2）确保数据的及时更新（一周进行一次全盘更新）
    （3）数据涵盖尽可能多的公司
    （4）数据检索快速性，读取数据等操作达到系统性能需求
    （5）操作简便性，界面简单便捷，增强用户体验
    4.1.2爬虫模块设计思想
    （1）定期对爬虫模块进行检验维护，确保数据获取完整，进行定期抽检对比数据库中已获取数据与国家企业信用管理系统数据。同时可以在控制台查看爬虫运行情况。
    （2）为了确保数据的及时更新，应当提高数据的获取速度，采用hadoop集群部署水平扩展的分布式爬虫对全国不同省份同时进行爬取，这样可以大大加快爬虫爬取效率，同时还可以在一定程度上减少IP被封的情况。
    （3）为了有尽可能多的公司数据，首先得获得最全的企业名录，只有了公司名字才能在国家企业信用管理系统上获取到信息。企业名录主要在买购网、黄页网等网站上爬取，在爬虫运行过程中实时从数据库中去掉一些无用、虚假的企业，最后再定时对企业名录数据库进行更新。
    （4）验证码攻防之路是不会停歇的，极验等验证码平台一定会不停加强防范措施，所以后台工程师应当定时检查爬虫模块有效性，发现问题及时调整。
    这种验证码破解相对简单，主要步骤如下：
    （1）爬取原图。
    （2）爬取缺口图。
    （3）将原来的图片与缺口图片进行比较，根据像素点区别情况识别出缺口的位置
    （4）然后再调用selenium自动化测试框架的拖动模块按照模拟人类行为的拖动函数拖动滑块缺口位置，破解成功！
    这种验证码的破解就相对困难了，需要在图中依次点击汉字，这对于人来说并不难，但对于爬虫来说难度很大。我们的解决方案如下： 
    （1）先通过yolo神经网络训练定位器模型，用该模型将图片中汉字的位置精确定位出来
    （2）然后再根据汉字位置截下定位出的汉字，通过darnet训练出的分类器模型识别汉字，再与提示词中的汉字进行比较，确定好点击顺序。
    （3）用selenium自动化测试框架按照（2）中确定好的顺序及位置点击图片————>通过验证!
  4.2离线数据处理方案
    4.2.1数据传输
    通过sqoop实现定期的hbase以及mysql的数据交互传输，增强数据库的可扩展性
    4.2.2集群应用
    启动spark集群中的master，worker，在本地8080端口可以看到当前的节点任务的状态
    4.2.3执行分析程序
    启动另外一个窗口进行提交作业，执行spark-submit，执行我们的数据分析处理的代码，图4.2.4中为数据离线处理中以dataframe呈现的数据结构图，并且
    最终将我们的分析结果存储到mysql中。我们通过正则表达式从爬取到的数据中匹配到我们需要的数据存到新的表内，在具体分析的时候利用rdd一系列  transformation及action操作（map，reducebykey...,）以及进一步的查询进行分层的结果获取并将数据存到mysql中，对于疑似控制人的判定，我们先提取当前最大  股东，若最大控股股东为直接入股的股东则该股东为疑似控制人，若为通过公司间接投资，则一级一级的向上遍历，直到实际控股人为股东为。在实际使用的时候，我们再代码中写入了Sparkstreming的监听程序，监听 file:///home/hadoop/spark/mycode/streaming/logfile 中的文件，设定监听时间为20S一次，当我们把爬下来的数据转成txt文件存到相应目录时，Sparkstreming检测到目标文件会自动执行分析程序，并将分析结果存到mysql中。
  4.3集群部署方案
  我们部署了spark，hadoop，hbase等集群，方便大规模的数据的离线处理，利用hbase 的高性能读写以及spark快速分析性能，让我们的数据处理更加快速精准。




